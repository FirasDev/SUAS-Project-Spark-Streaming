{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming on Africa's Political Conflicts Dataset\n",
    "\n",
    "### You should run the Kafka publisher .py file to stream data into the topic \"conflicts\"\n",
    "#### this notebook should be run by pyspark using this command :\n",
    "PYSPARK_PYTHON=python3 $SPARK_HOME/bin/pyspark --jars spark-sql-kafka-0-10_2.12-3.0.0.jar,spark-streaming-kafka-0-10-assembly_2.12-3.0.0.jar,kafka-clients-0.10.2.2.jar,commons-pool2-2.8.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This commented code was used to devide the dataset into parts before switching to Kafka\n",
    "#import pandas as pd\n",
    "\n",
    "#in_csv = 'conflicts.csv'\n",
    "#number_lines = sum(1 for row in (open(in_csv)))\n",
    "#rowsize = 5000\n",
    "#for i in range(1,number_lines,rowsize):\n",
    "#    df = pd.read_csv(in_csv,\n",
    "#          header=None,\n",
    "#          nrows = rowsize,#number of rows to read at each loop\n",
    "#          skiprows = i)\n",
    "#    out_csv = 'conflicts' + str(i) + '.csv'\n",
    "#    df.to_csv(out_csv,\n",
    "#          index=False,\n",
    "#          header=False,\n",
    "#          mode='a',#append data to csv file\n",
    "#          chunksize=rowsize)#size of data to append for each loop\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#Create the session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ConflictsApp\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read the stream from kafka topic conflicts \n",
    "dfraw = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"subscribe\", \"conflicts\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .load()\n",
    "dfraw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- : string (nullable = true)\n",
      " |-- ISO: string (nullable = true)\n",
      " |-- EVENT_DATE: string (nullable = true)\n",
      " |-- EVENT_TYPE: string (nullable = true)\n",
      " |-- SUB_EVENT_TYPE: string (nullable = true)\n",
      " |-- ACTOR1: string (nullable = true)\n",
      " |-- ASSOC_ACTOR_1: string (nullable = true)\n",
      " |-- ACTOR2: string (nullable = true)\n",
      " |-- ASSOC_ACTOR_2: string (nullable = true)\n",
      " |-- INTERACTION: string (nullable = true)\n",
      " |-- REGION: string (nullable = true)\n",
      " |-- COUNTRY: string (nullable = true)\n",
      " |-- ADMIN1: string (nullable = true)\n",
      " |-- ADMIN2: string (nullable = true)\n",
      " |-- LOCATION: string (nullable = true)\n",
      " |-- SOURCE: string (nullable = true)\n",
      " |-- NOTES: string (nullable = true)\n",
      " |-- FATALITIES: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create the schema for our data\n",
    "schema = StructType([StructField(\"\", StringType(),True),\n",
    "                         StructField(\"ISO\", StringType(),True),\n",
    "                         StructField(\"EVENT_DATE\", StringType(),True),\n",
    "                         StructField(\"EVENT_TYPE\", StringType(),True),\n",
    "                         StructField(\"SUB_EVENT_TYPE\", StringType(),True),\n",
    "                         StructField(\"ACTOR1\", StringType(),True),\n",
    "                         StructField(\"ASSOC_ACTOR_1\", StringType(),True),\n",
    "                         StructField(\"ACTOR2\", StringType(),True),\n",
    "                         StructField(\"ASSOC_ACTOR_2\", StringType(),True),\n",
    "                         StructField(\"INTERACTION\", StringType(),True),\n",
    "                         StructField(\"REGION\", StringType(),True),\n",
    "                         StructField(\"COUNTRY\", StringType(),True),\n",
    "                         StructField(\"ADMIN1\", StringType(),True),\n",
    "                         StructField(\"ADMIN2\", StringType(),True),\n",
    "                         StructField(\"LOCATION\", StringType(),True),\n",
    "                         StructField(\"SOURCE\", StringType(),True),\n",
    "                         StructField(\"NOTES\", StringType(),True),\n",
    "                         StructField(\"FATALITIES\", StringType(),True)\n",
    "                    ])\n",
    "\n",
    "#convert the received data as defined by our schema and keeping the timestamp\n",
    "df = dfraw.selectExpr(\"CAST(value AS STRING)\", \"CAST(timestamp AS TIMESTAMP)\") \\\n",
    "  .select(from_json(\"value\", schema).alias(\"data\"), \"timestamp\") \\\n",
    "  .select(\"data.*\", \"timestamp\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f9c46b7b340>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visualize the data recieved\n",
    "df.writeStream.format(\"console\").outputMode(\"append\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- : string (nullable = true)\n",
      " |-- ISO: string (nullable = true)\n",
      " |-- EVENT_DATE: timestamp (nullable = true)\n",
      " |-- EVENT_TYPE: string (nullable = true)\n",
      " |-- SUB_EVENT_TYPE: string (nullable = true)\n",
      " |-- ACTOR1: string (nullable = true)\n",
      " |-- ASSOC_ACTOR_1: string (nullable = true)\n",
      " |-- ACTOR2: string (nullable = true)\n",
      " |-- ASSOC_ACTOR_2: string (nullable = true)\n",
      " |-- INTERACTION: string (nullable = true)\n",
      " |-- REGION: string (nullable = true)\n",
      " |-- COUNTRY: string (nullable = true)\n",
      " |-- ADMIN1: string (nullable = true)\n",
      " |-- ADMIN2: string (nullable = true)\n",
      " |-- LOCATION: string (nullable = true)\n",
      " |-- SOURCE: string (nullable = true)\n",
      " |-- NOTES: string (nullable = true)\n",
      " |-- FATALITIES: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ds = df.toDF(*df.columns)\n",
    "#Converting FATALITIES to Integer and EVENT_DATE to timestamp type\n",
    "ds = df.withColumn(\"FATALITIES\", df[\"FATALITIES\"].cast(IntegerType()))\n",
    "ds = df.withColumn(\"EVENT_DATE\", to_timestamp(df[\"EVENT_DATE\"],\"yyyy-MM-dd\"))\n",
    "to_timestamp\n",
    "ds.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f9c46ba8a90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = ds.groupBy(\"REGION\").count()\n",
    "query \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f9c46c43940>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = ds.groupBy(\"INTERACTION\").sum(\"FATALITIES\")\n",
    "query \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f9c46b8e100>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = ds.select(date_format('EVENT_DATE','yyyy-MM').alias('month')).filter(col('INTERACTION').contains(\"2\")).groupBy(\"month\").count()\n",
    "query \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f9c46cbe700>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = ds.filter(col('INTERACTION').contains(\"3\")).groupBy(year('EVENT_DATE')).count()\n",
    "query \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowedCounts = ds \\\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(ds.timestamp, \"10 minutes\", \"5 minutes\"),\n",
    "        ds.ISO) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fdb3d82d760>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windowedCounts.writeStream.format(\"console\").outputMode(\"update\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
